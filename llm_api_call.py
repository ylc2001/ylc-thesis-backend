from handyllm import OpenAIClient
from handyllm import PromptConverter, stream_chat
import os
import json
import yaml
from zhipuai import ZhipuAI
import textwrap
import time


# 说明：如果测试新的题目数据，输入要放到step_1最下面，$user$里面
# 之后的过程是自动化的
# 万一最后格式有问题没有输出到json文件，去终端拷贝输出手动搞
# 每一步耗时在 30s 左右，总流程一分半，别急

input_folder = "input_data_yaml"
input_filename = "0424_cwh_correct"
thinkaloud_chunk = False 	        # 原始的语音转文字文本是否分段处理
llm_api_name = "gpt"                # "gpt" or "glm"
use_existing_data_yaml = True      # 是否使用 yaml 文件中已有的中间结果

std_graph_path = os.path.join(os.getcwd(), input_folder, "standard_graph.json")
input_path = os.path.join(os.getcwd(), input_folder, input_filename+".yaml")
cache_path = os.path.join(os.getcwd(), input_folder, input_filename+"_cache.yaml")

with open('config.yaml', 'r') as file:
	config = yaml.safe_load(file)
with open(input_path, 'r') as file:
	input_data = yaml.safe_load(file)
# if cache_path exists, load it
if os.path.exists(cache_path) and use_existing_data_yaml:
	with open(cache_path, 'r') as file:
		input_data = yaml.safe_load(file)
with open(std_graph_path, 'r') as json_file:
    standard_graph = json.dumps(json.load(json_file))

# input_data: problem, think_aloud, written_text; audio_text

# promt 文件存在 ./dir_name/step_x.txt
dir_name = "prompts"
audio_text_promt = "audio_text.txt"
step_1 = "1_check_correctness.txt"
step_2 = "2_memory.txt"
step_3 = "3_computation_graph.txt"
step_6 = "6_standardize.txt"

def call_llm_api(chat: list, api_name: str, json_format=False):
	'''
	chat: list, chat format generated by PromptConverter

	return: str, response from Azure API
	'''
	while True:
		try:
			if api_name == "gpt":
				with OpenAIClient(
					api_key=config["azure_api_key"], 
					api_base='https://pcg-west-us.openai.azure.com/', 
					api_type='azure', 
					api_version='2023-12-01-preview'  # 这个是哪来的？不敢动
					) as client:
					response = client.chat(
						model="gpt-4-1106-preview",  # 注意模型名字是azure里定义的名字，不一定是这个
						messages=chat,
						stream=True,
						response_format={ "type": "json_object" if json_format else "text"},
						temperature=0.2,
					).call()  ## note .call() here
					result = ""
					print("---------- Azure API response ----------")
					for text in stream_chat(response):
						result += text
						print(text, end='')
					print("\n----------------------------------------")
					return result
			elif api_name == "glm":
				client = ZhipuAI(api_key=config["zhipuai_api_key"])
				response = client.chat.completions.create(
						model="glm-4",  # 填写需要调用的模型名称
						messages=chat,
						stream=True,
					)
				result = ""
				print("---------- ChatGLM API response ----------")
				for (i, chunk) in enumerate(response):
					result += chunk.choices[0].delta.content
					print(chunk.choices[0].delta.content, end='')
				print("\n----------------------------------------")
				return result
			else:
				print("API name not recognized.")
				break
		except Exception as e:
			print(f"API调用失败，错误信息：{e}。5秒后重试。")
			time.sleep(5)  # 等待5秒再次尝试
	

converter = PromptConverter()

if "audio_text" in input_data and use_existing_data_yaml:
	print(">>> Using existing [fixed audio_text] <<<")
else:
	chat_audio = converter.rawfile2chat(os.path.join(os.getcwd(), dir_name, audio_text_promt))
	if thinkaloud_chunk:
		think_aloud_parts = textwrap.wrap(input_data["think_aloud"], 500)
		results = []
		for part in think_aloud_parts:
			print(part)
			chat_audio = converter.rawfile2chat(os.path.join(os.getcwd(), dir_name, audio_text_promt))
			chat_audio[-1]["content"] = chat_audio[-1]["content"].replace("%think_aloud%", part)
			print(">>> FIX AUDIO TEXT (part) <<<")
			result = call_llm_api(chat_audio, llm_api_name)
			results.append(result)
		input_data["audio_text"] = "".join(results)
	else:
		chat_audio[-1]["content"] = chat_audio[-1]["content"].replace("%think_aloud%", input_data["think_aloud"])
		print(">>> FIX AUDIO TEXT <<<")
		input_data["audio_text"] = call_llm_api(chat_audio, llm_api_name)
 

# Step 1, check_correctness 检查标准计算图中每一步的正确性
# if "checked_std_graph" in input_data and use_existing_data_yaml:
# 	print(">>> Using existing [check_correctness] <<<")
# else:
chat_1 = converter.rawfile2chat(os.path.join(os.getcwd(), dir_name, step_1))
chat_1[-1]["content"] = chat_1[-1]["content"].replace("%problem%", input_data["problem"])
chat_1[-1]["content"] = chat_1[-1]["content"].replace("%audio_text%", input_data["audio_text"])
chat_1[-1]["content"] = chat_1[-1]["content"].replace("%written_text%", input_data["written_text"])
chat_1[-1]["content"] = chat_1[-1]["content"].replace("%standard_graph%", standard_graph)
print(">>> Step 1 check_correctness <<<")
input_data["checked_std_graph"] = call_llm_api(chat_1, llm_api_name, True)

# 保存标注后的计算图到 json
output_json_path = os.path.join(os.getcwd(), "results", input_filename+"_checked_std.json")
with open(output_json_path, 'w') as file:
	try:
		input_data["checked_std_graph"] = input_data["checked_std_graph"].replace("```json", "")
		input_data["checked_std_graph"] = input_data["checked_std_graph"].replace("```", "")
		json.dump(json.loads(input_data["checked_std_graph"]), file, indent=4, ensure_ascii=False)
		print(f">>> 标注后的计算图 saved to json file.")
	except Exception as e:
		print("!! 标注后的计算图 is not a json string.")
		print(e)






# write into yaml file
with open(cache_path, 'w') as file:
	yaml.dump(input_data, file, default_flow_style=False, allow_unicode=True)
	print(f">>> results saved to cache yaml file.")