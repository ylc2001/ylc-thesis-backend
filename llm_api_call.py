from handyllm import OpenAIClient
from handyllm import PromptConverter, stream_chat
import os
import json
import yaml
from zhipuai import ZhipuAI
import textwrap
import time


# 说明：如果测试新的题目数据，输入要放到step_1最下面，$user$里面
# 之后的过程是自动化的
# 万一最后格式有问题没有输出到json文件，去终端拷贝输出手动搞
# 每一步耗时在 30s 左右，总流程一分半，别急

input_folder = "input_data_yaml"
input_filename = "0424_cwh_wrong.yaml"
thinkaloud_chunk = False 	        # 原始的语音转文字文本是否分段处理
llm_api_name = "gpt"                # "gpt" or "glm"
use_existing_data_yaml = False      # 是否使用 yaml 文件中已有的中间结果

input_path = os.path.join(os.getcwd(), input_folder, input_filename)

with open('config.yaml', 'r') as file:
	config = yaml.safe_load(file)
with open(input_path, 'r') as file:
	input_data = yaml.safe_load(file)

# input_data: problem, think_aloud, written_text; audio_text

# promt 文件存在 ./dir_name/step_x.txt
dir_name = "prompts"
audio_text_promt = "audio_text.txt"
step_1 = "1_action_list.txt"
step_2 = "2_memory.txt"
step_3 = "3_computation_graph.txt"
step_6 = "6_standardize.txt"

def call_llm_api(chat: list, api_name: str, json_format=False):
	'''
	chat: list, chat format generated by PromptConverter

	return: str, response from Azure API
	'''
	while True:
		try:
			if api_name == "gpt":
				with OpenAIClient(
					api_key=config["azure_api_key"], 
					api_base='https://pcg-west-us.openai.azure.com/', 
					api_type='azure', 
					api_version='2023-12-01-preview'  # 这个是哪来的？不敢动
					) as client:
					response = client.chat(
						model="gpt-4-1106-preview",  # 注意模型名字是azure里定义的名字，不一定是这个
						messages=chat,
						stream=True,
						response_format={ "type": "json_object" if json_format else "text"}
					).call()  ## note .call() here
					result = ""
					print("---------- Azure API response ----------")
					for text in stream_chat(response):
						result += text
						print(text, end='')
					print("\n----------------------------------------")
					return result
			elif api_name == "glm":
				client = ZhipuAI(api_key=config["zhipuai_api_key"])
				response = client.chat.completions.create(
						model="glm-4",  # 填写需要调用的模型名称
						messages=chat,
						stream=True,
					)
				result = ""
				print("---------- ChatGLM API response ----------")
				for (i, chunk) in enumerate(response):
					result += chunk.choices[0].delta.content
					print(chunk.choices[0].delta.content, end='')
				print("\n----------------------------------------")
				return result
			else:
				print("API name not recognized.")
				break
		except Exception as e:
			print(f"API调用失败，错误信息：{e}。5秒后重试。")
			time.sleep(5)  # 等待5秒再次尝试
	

converter = PromptConverter()

if "audio_text" in input_data and use_existing_data_yaml:
	print(">>> Using existing [fixed audio_text] <<<")
else:
	chat_audio = converter.rawfile2chat(os.path.join(os.getcwd(), dir_name, audio_text_promt))
	if thinkaloud_chunk:
		think_aloud_parts = textwrap.wrap(input_data["think_aloud"], 500)
		results = []
		for part in think_aloud_parts:
			print(part)
			chat_audio = converter.rawfile2chat(os.path.join(os.getcwd(), dir_name, audio_text_promt))
			chat_audio[-1]["content"] = chat_audio[-1]["content"].replace("%think_aloud%", part)
			print(">>> FIX AUDIO TEXT (part) <<<")
			result = call_llm_api(chat_audio, llm_api_name)
			results.append(result)
		input_data["audio_text"] = "".join(results)
	else:
		chat_audio[-1]["content"] = chat_audio[-1]["content"].replace("%think_aloud%", input_data["think_aloud"])
		print(">>> FIX AUDIO TEXT <<<")
		input_data["audio_text"] = call_llm_api(chat_audio, llm_api_name)
 

# Step 1, action list 动作列表
if "action_list" in input_data and use_existing_data_yaml:
	print(">>> Using existing [action list] <<<")
else:
	chat_1 = converter.rawfile2chat(os.path.join(os.getcwd(), dir_name, step_1))
	chat_1[-1]["content"] = chat_1[-1]["content"].replace("%problem%", input_data["problem"])
	chat_1[-1]["content"] = chat_1[-1]["content"].replace("%audio_text%", input_data["audio_text"])
	chat_1[-1]["content"] = chat_1[-1]["content"].replace("%written_text%", input_data["written_text"])
	print(">>> Step 1 action_list <<<")
	input_data["action_list"] = call_llm_api(chat_1, llm_api_name)

# Step 2, memory
if "memory" in input_data and use_existing_data_yaml:
	print(">>> Using existing [memory] <<<")
else:
	chat_2 = converter.rawfile2chat(os.path.join(os.getcwd(), dir_name, step_2))
	chat_2[-1]["content"] = chat_2[-1]["content"].replace("%problem%", input_data["problem"])
	chat_2[-1]["content"] = chat_2[-1]["content"].replace("%audio_text%", input_data["audio_text"])
	chat_2[-1]["content"] = chat_2[-1]["content"].replace("%written_text%", input_data["written_text"])
	chat_2[-1]["content"] = chat_2[-1]["content"].replace("%action_list%", input_data["action_list"])
	print(">>> Step 2 memory <<<")
	input_data["memory"] = call_llm_api(chat_2, llm_api_name)

# Step 3, student_graph 学生的计算图 初版
if "student_graph" in input_data and use_existing_data_yaml:
	print(">>> Using existing [student_graph] <<<")
else:
	chat_3 = converter.rawfile2chat(os.path.join(os.getcwd(), dir_name, step_3))
	chat_3[-1]["content"] = chat_3[-1]["content"].replace("%problem%", input_data["problem"])
	chat_3[-1]["content"] = chat_3[-1]["content"].replace("%audio_text%", input_data["audio_text"])
	chat_3[-1]["content"] = chat_3[-1]["content"].replace("%written_text%", input_data["written_text"])
	chat_3[-1]["content"] = chat_3[-1]["content"].replace("%memory%", input_data["memory"])
	print(">>> Step 3 student_graph <<<")
	input_data["student_graph"] = call_llm_api(chat_3, llm_api_name)

output_json_path = os.path.join(os.getcwd(), "results", input_filename+"_graph.json")
with open(output_json_path, 'w') as file:
	try:
		json.dump(json.loads(input_data["student_graph"]), file, indent=4, ensure_ascii=False)
		print(f">>> Graph results saved to json file.")
	except Exception as e:
		print("!! Graph result is not a json string.")
		print(e)

# Step 6, student_graph_std 学生计算图标准化
if "student_graph_std" in input_data and use_existing_data_yaml:
	print(">>> Using existing [student_graph_std] <<<")
else:
	standard_graph = json.load(open(os.path.join(os.getcwd(), "results", "standard_graph.json"), 'r'))
	standard_graph_str = json.dumps(standard_graph, indent=4, ensure_ascii=False)
	chat_6 = converter.rawfile2chat(os.path.join(os.getcwd(), dir_name, step_6))
	chat_6[-1]["content"] = chat_6[-1]["content"].replace("%student_graph%", input_data["student_graph"])
	chat_6[-1]["content"] = chat_6[-1]["content"].replace("%standard_graph%", standard_graph_str)
	print(">>> Step 6 student_graph_std <<<")
	input_data["student_graph_std"] = call_llm_api(chat_6, llm_api_name)

output_json_path = os.path.join(os.getcwd(), "results", input_filename+"_graph_std.json")
with open(output_json_path, 'w') as file:
	try:
		input_data["student_graph_std"] = input_data["student_graph_std"].replace("```json", "")
		input_data["student_graph_std"] = input_data["student_graph_std"].replace("```", "")
		json.dump(json.loads(input_data["student_graph_std"]), file, indent=4, ensure_ascii=False)
		print(f">>> Graph results saved to json file.")
	except Exception as e:
		print("!! Graph result is not a json string.")
		print(e)





# write into yaml file
with open(input_path, 'w') as file:
	yaml.dump(input_data, file, default_flow_style=False, allow_unicode=True)
	print(f">>> results saved to yaml file.")